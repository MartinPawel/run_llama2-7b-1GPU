{
    "n_epochs": 2,
    "batch_size": 1,
    "gradient_accumulation_steps": 4,
    "ngpu": 1,
    "learning_rate": 5e-5,
    "model_name_or_path": "huggyllama/llama-7b",
    "max_length": 1024,
    "num_workers": 64,
    "fp16": true,
    "u_bs": 1,
    "K_models": 10,
    "in_prob": 0.5,
    "rng_offset": 0,
    "path_prefix_model": "path/to/model",
    "path_prefix_data": "path/to/data"
}